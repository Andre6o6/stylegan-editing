{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN_edit_images",
      "provenance": [],
      "authorship_tag": "ABX9TyOZx4F88dj055V5dfcaY5kO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andre6o6/stylegan-editing/blob/master/StyleGAN_edit_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoPAasACQnUV"
      },
      "source": [
        "# This notebook shows the example of image editing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2vdpvWgLwD4"
      },
      "source": [
        "#@title Load dependences {display-mode: \"form\"}\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "\n",
        "!git clone https://github.com/Andre6o6/stylegan-editing.git stylegan_editing\n",
        "%cd stylegan_editing\n",
        "\n",
        "#TODO as submodule\n",
        "!git clone https://github.com/genforce/interfacegan.git\n",
        "\n",
        "# Load pretrained models\n",
        "!gdown --id 1r3Qygz6DaXtQwkUbd35ucA2U4hayj32m\n",
        "!mv karras2019stylegan-ffhq-1024x1024.pkl interfacegan/models/pretrain/\n",
        "!gdown --id 1C9MSghPDWnkccGXgU6S9-wnRgPVBVovL\n",
        "\n",
        "!pip install facenet-pytorch\n",
        "\n",
        "!pip install ffmpeg-python\n",
        "!pip install scikit-video\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5BVCOWpyam1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZDxWvNvRXSW",
        "outputId": "43fc8ded-9c4c-4035-f276-390c370bd57b",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 39
        }
      },
      "source": [
        "#@title Upload your images {display-mode: \"form\"}\n",
        "\n",
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "os.makedirs(\"latents\", exist_ok=True)\n",
        "os.makedirs(\"raw_images\", exist_ok=True)\n",
        "os.makedirs(\"aligned_images\", exist_ok=True)\n",
        "\n",
        "uploaded = files.upload()\n",
        "for fn in uploaded.keys():\n",
        "    os.rename(fn, os.path.join(\"raw_images\", fn))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f6573acc-0d78-425e-be57-ebe7e93b6e01\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f6573acc-0d78-425e-be57-ebe7e93b6e01\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0hjfo2NVJg-"
      },
      "source": [
        "#@title (Optional) or download images, picked from FEI Face Database {display-mode: \"form\"}\n",
        "\n",
        "!mkdir latents/ raw_images/ aligned_images/\n",
        "\n",
        "%cd raw_images/\n",
        "!gdown --id 1qnf8pbtTmBguMUAre0ZeC1F_DqpWnFcp\n",
        "#!gdown --id 1eFKVKEGsGr9Yl-uI3lC3XaqthAX8uPl4\n",
        "!gdown --id 1KzXBdEvTyhyyWuPwJpGC05Ix6HU1foVm\n",
        "#!gdown --id 1vPOidmfgxvsks3hfqOya_WfNElkwlcDo\n",
        "#!gdown --id 1hkQZH8DoPioUMkr60S5MM3OjYlJgvnK5\n",
        "!gdown --id 1Br3jI1ae2T0eN0TNHDfistlb1fshLQdR\n",
        "%cd ..\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKdiFdNbyXgQ"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNNP68nZOe9r"
      },
      "source": [
        "#@title Align images {display-mode: \"form\"}\n",
        "\n",
        "!python align/align_images.py raw_images/ aligned_images/ --output_size=1024\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9bErZwMyUOu"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H12C5mD5dzj6"
      },
      "source": [
        "#@title Encode images in StyleGAN latent space... {display-mode: \"form\"}\n",
        "#@markdown This will take some time.\n",
        "#@markdown We set number of iterations to 400 to get decent quality on real images, so it takes around 3 min per image. \n",
        "\n",
        "!python encode_images.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clBZ4PMdhkaD"
      },
      "source": [
        "#@title ...and show results: {display-mode: \"form\"}\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "\n",
        "from interfacegan.models.stylegan_generator import StyleGANGenerator\n",
        "from models.latent_optimization import LatentOptimizer\n",
        "from utils.io import get_image_numpy\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "converted_model = StyleGANGenerator(\"stylegan_ffhq\")\n",
        "latent_optimizer = LatentOptimizer(converted_model.model)\n",
        "\n",
        "print(\"\\n\\t\\tInput\\t\\tReconstruction\")\n",
        "for imgname in os.listdir(\"aligned_images\"):\n",
        "    img_path = \"aligned_images/\" + imgname\n",
        "    latent_path = \"latents/\" + imgname + \".npy\"\n",
        "\n",
        "    img = np.array(Image.open(img_path))\n",
        "\n",
        "    latent = np.load(latent_path)\n",
        "    _, generated_image = latent_optimizer(torch.from_numpy(latent).to(device))\n",
        "    g = get_image_numpy(generated_image)\n",
        "\n",
        "    imgs_comb = np.hstack((img, g))\n",
        "    display(Image.fromarray(imgs_comb).resize((512,256)))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7u-8n5myPw9"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HX8O7ZuGtque"
      },
      "source": [
        "## Edit image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vm8aImYxUfIW",
        "cellView": "form"
      },
      "source": [
        "#@title Run UI\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import ipywidgets as widgets\n",
        "\n",
        "#Dropdown lists to choose images\n",
        "img_list = os.listdir(\"aligned_images/\")\n",
        "input_picker = widgets.Dropdown(options=img_list)\n",
        "exemplar_picker = widgets.Dropdown(options=img_list)\n",
        "feature_picker = widgets.Dropdown(options=[\"pose\", \"smile\"], value=\"pose\")\n",
        "\n",
        "print(\"Choose exemplar image, from which feature will be transfered:\")\n",
        "display(exemplar_picker)\n",
        "print(\"Choose your target image for feature transfer:\")\n",
        "display(input_picker)\n",
        "print(\"Choose feature to transfer:\")\n",
        "display(feature_picker)\n",
        "\n",
        "#Interactive display that shows chosen images\n",
        "def display_feature_ui(exemplar, input, feature):\n",
        "    print(f\"{feature} transfer: {exemplar} --> {input}\")\n",
        "    paths = [\"aligned_images/\"+x for x in [exemplar, input]]\n",
        "    imgs = [Image.open(x).resize((256,256)) for x in paths]\n",
        "    imgs_comb = np.hstack( [np.asarray(x) for x in imgs] )\n",
        "    display(Image.fromarray(imgs_comb))\n",
        "\n",
        "out = widgets.interactive_output(\n",
        "    display_feature_ui, \n",
        "    {\"exemplar\": exemplar_picker, \"input\": input_picker, \"feature\": feature_picker}\n",
        ")\n",
        "\n",
        "display(out)\n",
        "\n",
        "#Strength slider\n",
        "print(\"Adjust edit strength:\")\n",
        "scale = widgets.FloatSlider(min=0, max=5, step=0.05, value=1, continuous_update=False)\n",
        "display(scale)\n",
        "print(\"Adjust identity preservation strength:\")\n",
        "print(\"(Bigger values tend to produse better results, but lessen feature transfer strength.)\")\n",
        "identity = widgets.FloatSlider(min=1, max=10, step=0.5, value=5, continuous_update=False)\n",
        "display(identity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSAnHD0j4UYz"
      },
      "source": [
        "#@title Perform feature transfer... {display-mode: \"form\"}\n",
        "#@markdown This will take some time.\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from IPython import display as ipythondisplay\n",
        "from IPython.display import display\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from facenet_pytorch import InceptionResnetV1\n",
        "from interfacegan.models.stylegan_generator import StyleGANGenerator\n",
        "from models.latent_optimization import LatentOptimizer\n",
        "from edit_images import morph_coefficient, feature_morph\n",
        "from utils.io import get_image_numpy\n",
        "\n",
        "# Load models\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "converted_model = StyleGANGenerator(\"stylegan_ffhq\")\n",
        "latent_optimizer = LatentOptimizer(converted_model.model, latent_space=\"WP\")\n",
        "facenet = InceptionResnetV1(pretrained='vggface2').to(device).eval()\n",
        "\n",
        "# Get feature vector\n",
        "if (feature_picker.value == \"smile\"):\n",
        "    boundary = np.load(\"boundaries/stylegan_ffhq_smile_w_boundary.npy\")\n",
        "elif (feature_picker.value == \"pose\"):\n",
        "    boundary = np.load(\"boundaries/stylegan_ffhq_pose_w_boundary.npy\")\n",
        "\n",
        "# Get latent vectors\n",
        "w_input = np.load(\"latents/\" + input_picker.value + \".npy\")\n",
        "w_exemplar = np.load(\"latents/\" + exemplar_picker.value + \".npy\")\n",
        "# Calculate edit distance\n",
        "effect_coef = 2 * scale.value * morph_coefficient(w_input, w_exemplar, boundary, map_k=5)\n",
        "\n",
        "# Show initial prediction, obtained with just linear shift\n",
        "print(\"\\nInitial rough feature transfer prediction (not final result):\")\n",
        "temp = (w_input + effect_coef*boundary).astype(np.float32)\n",
        "_, generated_image = latent_optimizer(torch.from_numpy(temp).to(device))\n",
        "g = get_image_numpy(generated_image)\n",
        "display(Image.fromarray(g).resize((256,256)))\n",
        "\n",
        "# Perform edit\n",
        "print(\"Performing feature transfer...\")\n",
        "res, path = feature_morph(w_input, boundary, effect_coef, latent_optimizer, facenet, \\\n",
        "                          identity_coef=identity.value)\n",
        "\n",
        "ipythondisplay.clear_output(wait=True)\n",
        "print(\"Finished.\")\n",
        "\n",
        "# Show final result image\n",
        "_, generated_image = latent_optimizer(torch.from_numpy(res).to(device))\n",
        "g = get_image_numpy(generated_image)\n",
        "display(Image.fromarray(g).resize((512,512)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKK8-5QVwezW"
      },
      "source": [
        "#@title (Optional) Show video {display-mode: \"form\"}\n",
        "\n",
        "\n",
        "from skimage.transform import resize\n",
        "import skvideo.io\n",
        "from moviepy.editor import VideoFileClip\n",
        "\n",
        "def gen(x):\n",
        "    _, generated_image = latent_optimizer(torch.from_numpy(x).to(device))\n",
        "    g = get_image_numpy(generated_image, saturate=True)\n",
        "    return resize(g,(512,512))\n",
        "\n",
        "frames = [gen(x) for x in path]\n",
        "frames = (255*np.array(frames)).astype(np.uint8)\n",
        "skvideo.io.vwrite(\"temp_out.mp4\", frames)\n",
        "\n",
        "clip = VideoFileClip(\"temp_out.mp4\")\n",
        "clip.ipython_display(height=512, autoplay=1, loop=1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}