{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StyleGAN pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMP/fkmeuL6PRO0noiuHe97",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andre6o6/stylegan-editing/blob/master/StyleGAN_encode_images.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMOXQTtc11Z-",
        "colab_type": "code",
        "outputId": "9df3f47b-8647-4776-b3a1-23d186aa7e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "!git clone https://github.com/genforce/interfacegan.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'interfacegan'...\n",
            "remote: Enumerating objects: 223, done.\u001b[K\n",
            "Receiving objects:   0% (1/223)   \rReceiving objects:   1% (3/223)   \rReceiving objects:   2% (5/223)   \rReceiving objects:   3% (7/223)   \rReceiving objects:   4% (9/223)   \rReceiving objects:   5% (12/223)   \rReceiving objects:   6% (14/223)   \rReceiving objects:   7% (16/223)   \rReceiving objects:   8% (18/223)   \rReceiving objects:   9% (21/223)   \rReceiving objects:  10% (23/223)   \rReceiving objects:  11% (25/223)   \rReceiving objects:  12% (27/223)   \rReceiving objects:  13% (29/223)   \rReceiving objects:  14% (32/223)   \rReceiving objects:  15% (34/223)   \rReceiving objects:  16% (36/223)   \rReceiving objects:  17% (38/223)   \rReceiving objects:  18% (41/223)   \rReceiving objects:  19% (43/223)   \rReceiving objects:  20% (45/223)   \rReceiving objects:  21% (47/223)   \rReceiving objects:  22% (50/223)   \rReceiving objects:  23% (52/223)   \rReceiving objects:  24% (54/223), 92.01 KiB | 168.00 KiB/s   \rReceiving objects:  25% (56/223), 92.01 KiB | 168.00 KiB/s   \rReceiving objects:  26% (58/223), 92.01 KiB | 168.00 KiB/s   \rReceiving objects:  27% (61/223), 92.01 KiB | 168.00 KiB/s   \rReceiving objects:  28% (63/223), 92.01 KiB | 168.00 KiB/s   \rReceiving objects:  29% (65/223), 92.01 KiB | 168.00 KiB/s   \rReceiving objects:  30% (67/223), 92.01 KiB | 168.00 KiB/s   \rReceiving objects:  31% (70/223), 92.01 KiB | 168.00 KiB/s   \rReceiving objects:  32% (72/223), 92.01 KiB | 168.00 KiB/s   \rReceiving objects:  33% (74/223), 92.01 KiB | 168.00 KiB/s   \rReceiving objects:  34% (76/223), 92.01 KiB | 168.00 KiB/s   \rReceiving objects:  34% (76/223), 868.01 KiB | 819.00 KiB/s   \rReceiving objects:  35% (79/223), 868.01 KiB | 819.00 KiB/s   \rReceiving objects:  36% (81/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  37% (83/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  38% (85/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  39% (87/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  40% (90/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  41% (92/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  42% (94/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  43% (96/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  44% (99/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  45% (101/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  46% (103/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  47% (105/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  48% (108/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  49% (110/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  50% (112/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  51% (114/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  52% (116/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  53% (119/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  54% (121/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  55% (123/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  56% (125/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  57% (128/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  58% (130/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  59% (132/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  60% (134/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  61% (137/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  62% (139/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  63% (141/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  64% (143/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  65% (145/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  66% (148/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  67% (150/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  68% (152/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  69% (154/223), 5.79 MiB | 3.65 MiB/s   \rremote: Total 223 (delta 0), reused 0 (delta 0), pack-reused 223\u001b[K\n",
            "Receiving objects:  70% (157/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  71% (159/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  72% (161/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  73% (163/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  74% (166/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  75% (168/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  76% (170/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  77% (172/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  78% (174/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  79% (177/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  80% (179/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  81% (181/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  82% (183/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  83% (186/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  84% (188/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  85% (190/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  86% (192/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  87% (195/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  88% (197/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  89% (199/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  90% (201/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  91% (203/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  92% (206/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  93% (208/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  94% (210/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  95% (212/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  96% (215/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  97% (217/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  98% (219/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects:  99% (221/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects: 100% (223/223), 5.79 MiB | 3.65 MiB/s   \rReceiving objects: 100% (223/223), 11.38 MiB | 5.71 MiB/s, done.\n",
            "Resolving deltas:   0% (0/70)   \rResolving deltas:   1% (1/70)   \rResolving deltas:  20% (14/70)   \rResolving deltas:  22% (16/70)   \rResolving deltas:  24% (17/70)   \rResolving deltas:  30% (21/70)   \rResolving deltas:  31% (22/70)   \rResolving deltas:  41% (29/70)   \rResolving deltas:  44% (31/70)   \rResolving deltas:  47% (33/70)   \rResolving deltas:  50% (35/70)   \rResolving deltas:  65% (46/70)   \rResolving deltas:  68% (48/70)   \rResolving deltas:  71% (50/70)   \rResolving deltas:  75% (53/70)   \rResolving deltas:  78% (55/70)   \rResolving deltas:  80% (56/70)   \rResolving deltas:  81% (57/70)   \rResolving deltas:  82% (58/70)   \rResolving deltas:  85% (60/70)   \rResolving deltas:  87% (61/70)   \rResolving deltas:  88% (62/70)   \rResolving deltas:  90% (63/70)   \rResolving deltas:  94% (66/70)   \rResolving deltas:  95% (67/70)   \rResolving deltas: 100% (70/70)   \rResolving deltas: 100% (70/70), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YJSB--VQGdd",
        "colab_type": "code",
        "outputId": "d9723722-73a6-4bf5-9a4d-a159e10b31ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!gdown https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ\n",
        "#!mv x.pkl interfacegan/models/pretrain/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Permission denied: https://drive.google.com/uc?id=1MEGjdvVpUsu1jB4zrXZN7Y4kBBOzizDQ\n",
            "Maybe you need to change permission over 'Anyone with the link'?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K97qeKma54q6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpfPbk0iF3vI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "def load_image(path):\n",
        "    image = np.asarray(Image.open(filename))\n",
        "    image = np.transpose(image, (2,0,1))  #WxHxC to CxWxH\n",
        "    return image\n",
        "\n",
        "def save_image(image, save_path):\n",
        "    image = np.transpose(image, (1,2,0)).astype(np.uint8)\n",
        "    image = Image.fromarray(image)\n",
        "    image.save(save_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWJM9weL48A5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision.models import vgg16\n",
        "\n",
        "def denormalize(synthesized_image, min_value=-1, max_value=1):\n",
        "    #Cast from [-1, 1] to [0, 255]; gradients should be ok ###(?)###\n",
        "    synthesized_image = 255. * (synthesized_image - min_value) / (max_value - min_value)\n",
        "    synthesized_image = torch.clamp(synthesized_image + 0.5, min=0, max=255)\n",
        "    return synthesized_image\n",
        "\n",
        "class VGGFeatureExtractor(nn.Module):\n",
        "    def __init__(self, vgg_layer=12):\n",
        "        super().__init__()\n",
        "        self.image_size = 256\n",
        "        self.mean = torch.tensor([0.485, 0.456, 0.406]).to(device).view(-1, 1, 1)\n",
        "        self.std = torch.tensor([0.229, 0.224, 0.225]).to(device).view(-1, 1, 1)\n",
        "\n",
        "        self.vgg16 = vgg16(pretrained=True).features[:vgg_layer].to(device).eval()\n",
        "\n",
        "    def forward(self, image):\n",
        "        image = image / 255.\n",
        "        image = F.adaptive_avg_pool2d(image, self.image_size)\n",
        "        image = (image - self.mean) / self.std\n",
        "        features = self.vgg16(image)\n",
        "        return features\n",
        "\n",
        "class LatentOptimizer(nn.Module):\n",
        "    def __init__(self, synthesizer, vgg_layer=9):\n",
        "        super().__init__()\n",
        "        self.synthesizer = synthesizer.to(device).eval()\n",
        "        self.feature_extractor = VGGFeatureExtractor(vgg_layer)\n",
        "\n",
        "    def forward(self, dlatents):\n",
        "        generated_image = self.synthesizer(dlatents)\n",
        "        generated_image = denormalize(generated_image)\n",
        "        features = self.feature_extractor(generated_image)\n",
        "        return features, generated_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97welj9xJYO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LatentLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.l1_loss = nn.L1Loss()\n",
        "        self.log_cosh_loss = LogCoshLoss()\n",
        "        self.l2_loss = nn.MSELoss()\n",
        "\n",
        "        self.vgg_loss_coef = 0.4\n",
        "        self.pixel_loss_coef = 1.5\n",
        "        self.l1_penalty = 0.3\n",
        "    \n",
        "    def forward(\n",
        "        self, \n",
        "        real_features, generated_features,\n",
        "        real_image=None, generated_image=None, \n",
        "        average_dlatents=None, dlatents=None,\n",
        "    ):           \n",
        "        loss = 0\n",
        "        # L1 loss on VGG16 features\n",
        "        if self.vgg_loss_coef != 0:\n",
        "            loss += self.vgg_loss_coef * self.l2_loss(real_features, generated_features)\n",
        "\n",
        "        # + logcosh loss on image pixels\n",
        "        if real_image is not None and generated_image is not None:\n",
        "            loss += self.pixel_loss_coef * self.log_cosh_loss(real_image, generated_image)\n",
        "\n",
        "        # Dlatent Loss - Forces latents to stay near the space the model uses for faces.\n",
        "        if average_dlatents is not None and dlatents is not None:\n",
        "            loss += self.l1_penalty * 512 * self.l1_loss(average_dlatents, dlatents)\n",
        "\n",
        "        return loss\n",
        "\n",
        "class LogCoshLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, true, pred):\n",
        "        loss = true - pred\n",
        "        return torch.mean(torch.log(torch.cosh(loss + 1e-12)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY_VI2hBTD9b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dlatent_path = \"latents/\"\n",
        "latent_predictor_path = \"\"\n",
        "image_path = \"\"\n",
        "predict_initial_approximation = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pu3-GXpN4M2q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from interfacegan.models.stylegan_generator import StyleGANGenerator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6o09AfV4Vya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "synthesizer = StyleGANGenerator(\"stylegan_ffhq\").model.synthesis\n",
        "latent_optimizer = LatentOptimizer(synthesizer, vgg_layer=9)\n",
        "\n",
        "# This shouldn't be needed if I don't pass them to optimizer(?)\n",
        "for param in latent_optimizer.parameters():\n",
        "    param.requires_grad_(False)\n",
        "\n",
        "reference_image = load_image(image_path)\n",
        "reference_image = torch.from_numpy(reference_image).unsqueeze(0).to(device)\n",
        "\n",
        "reference_features = latent_optimizer.feature_extractor(reference_image).detach()\n",
        "reference_image = reference_image.detach()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggb7Kj17HLbM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if predict_initial_approximation:\n",
        "    image_to_latent = InitialLatentPredictor().to(device)\n",
        "    image_to_latent.load_state_dict(torch.load(latent_predictor_path))\n",
        "    image_to_latent.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        initial_latents = image_to_latent(reference_image)\n",
        "    initial_latents = initial_latents.to(device).requires_grad_(True)\n",
        "else:\n",
        "    initial_latents = torch.zeros((1,18,512)).to(device).requires_grad_(True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdNs_0eDHdP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = LatentLoss()\n",
        "optimizer = torch.optim.Adam([initial_latents], lr=0.025)\n",
        "\n",
        "n_iters = 100\n",
        "progress_bar = tqdm(range(n_iters))\n",
        "for step in progress_bar:\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    generated_image_features, _ = latent_optimizer(initial_latents)\n",
        "    \n",
        "    loss = criterion(generated_image_features, reference_features)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    progress_bar.set_description(\"{}/{}: Loss = {}\".format(step+1, n_iters, loss.item()))\n",
        "\n",
        "optimized_dlatents = initial_latents.detach().cpu().numpy()\n",
        "np.save(dlatent_path, optimized_dlatents)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ls6GR6INPCkh",
        "colab_type": "code",
        "outputId": "6d2d156f-5074-46f4-e63b-3e943a2f0489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#avg_dlatents = StyleGANGenerator(\"stylegan_ffhq\").model.truncation.w_avg"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-06-14 14:30:04,755][WARNING] No pre-trained model will be loaded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7nSMRZlR3Eg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1UobQTGR3Hh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXbf4bqHT4_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEvAFTvCT5B7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3aOhV-1T5Em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ka6vChw-T5dN",
        "colab_type": "text"
      },
      "source": [
        "# Initial approximation prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95bj6K2UT5i-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchvision.models import resnet50\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "class InitialLatentPredictor(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.activation = torch.nn.ELU()\n",
        "\n",
        "        # 3, 256, 256 ->\n",
        "        self.resnet = list(resnet50(pretrained=True).children())[:-2]\n",
        "        self.resnet = torch.nn.Sequential(*self.resnet)\n",
        "        # -> 2048, 8, 8\n",
        "        self.conv2d = torch.nn.Conv2d(2048, 256, kernel_size=1)\n",
        "        self.flatten = torch.nn.Flatten()\n",
        "        self.dense1 = torch.nn.Linear(256*8*8, 512)\n",
        "        self.dense2 = torch.nn.Linear(512, (18 * 512))\n",
        "\n",
        "    def forward(self, image):\n",
        "        x = self.resnet(image)\n",
        "        x = self.conv2d(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense1(x)\n",
        "        x = self.dense2(x)\n",
        "        x = x.view((-1, 18, 512))\n",
        "        return x\n",
        "\n",
        "class ImageLatentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, filenames, dlatents, transforms = None):\n",
        "        self.filenames = filenames\n",
        "        self.dlatents = dlatents\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        filename = self.filenames[index]\n",
        "        dlatent = self.dlatents[index]\n",
        "\n",
        "        image = Image.open(filename)\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image, dlatent"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcjBJgq7cm92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from InterFaceGAN.models.stylegan_generator import StyleGANGenerator\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "from glob import glob\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0xNMOuMcsgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "augments = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "image_size = 256\n",
        "\n",
        "directory = \"../StyleGan/InterFaceGAN/test15/\"\n",
        "filenames = sorted(glob(directory + \"*.jpg\"))\n",
        "\n",
        "train_filenames = filenames[0:48000]\n",
        "validation_filenames = filenames[48000:]\n",
        "\n",
        "dlatents = np.load(directory + \"wp.npy\")\n",
        "\n",
        "train_dlatents = dlatents[0:48000]\n",
        "validation_dlatents = dlatents[48000:]\n",
        "\n",
        "train_dataset = ImageLatentDataset(train_filenames, train_dlatents, transforms=augments)\n",
        "validation_dataset = ImageLatentDataset(validation_filenames, validation_dlatents, transforms=augments)\n",
        "\n",
        "train_generator = torch.utils.data.DataLoader(train_dataset, batch_size=32)\n",
        "validation_generator = torch.utils.data.DataLoader(validation_dataset, batch_size=32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYEwvDkXdOre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_to_latent = InitialLatentPredictor(image_size).cuda()\n",
        "optimizer = torch.optim.Adam(image_to_latent.parameters())\n",
        "criterion = LogCoshLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXhmn-2ydYQm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 20\n",
        "validation_loss = 0.0\n",
        "\n",
        "progress_bar = tqdm(range(epochs))\n",
        "for epoch in progress_bar:    \n",
        "    running_loss = 0.0\n",
        "    \n",
        "    image_to_latent.train()\n",
        "    for i, (images, latents) in enumerate(train_generator, 1):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images, latents = images.cuda(), latents.cuda()\n",
        "        pred_latents = image_to_latent(images)\n",
        "        loss = criterion(pred_latents, latents)\n",
        "        loss.backward()\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        progress_bar.set_description(\"Step: {0}, Loss: {1:4f}, Validation Loss: {2:4f}\".format(i, running_loss / i, validation_loss))\n",
        "    \n",
        "    validation_loss = 0.0\n",
        "    \n",
        "    image_to_latent.eval()\n",
        "    for i, (images, latents) in enumerate(validation_generator, 1):\n",
        "        with torch.no_grad():\n",
        "            images, latents = images.cuda(), latents.cuda()\n",
        "            pred_latents = image_to_latent(images)\n",
        "            loss =  criterion(pred_latents, latents)\n",
        "            \n",
        "            validation_loss += loss.item()\n",
        "    \n",
        "    validation_loss /= i\n",
        "    progress_bar.set_description(\"Step: {0}, Loss: {1:4f}, Validation Loss: {2:4f}\".format(i, running_loss / i, validation_loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_BqorZpen8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(image_to_latent.state_dict(), \"./image_to_latent.pt\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQo3w06iequL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_to_latent = InitialLatentPredictor(image_size).cuda()\n",
        "image_to_latent.load_state_dict(torch.load(\"image_to_latent.pt\"))\n",
        "image_to_latent.eval()\n",
        "print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypmnao8zesGi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalized_to_normal_image(image):\n",
        "    mean=torch.tensor([0.485, 0.456, 0.406]).view(-1,1,1).float()\n",
        "    std=torch.tensor([0.229, 0.224, 0.225]).view(-1,1,1).float()\n",
        "    \n",
        "    image = image.detach().cpu()\n",
        "    \n",
        "    image *= std\n",
        "    image += mean\n",
        "    image *= 255\n",
        "    \n",
        "    image = image.numpy()[0]\n",
        "    image = np.transpose(image, (1,2,0))\n",
        "    return image.astype(np.uint8)\n",
        "\n",
        "\n",
        "num_test_images = 5\n",
        "images = [validation_dataset[i][0].unsqueeze(0).cuda() for i in range(num_test_images)]\n",
        "normal_images = list(map(normalized_to_normal_image, images))\n",
        "\n",
        "pred_dlatents = map(image_to_latent, images)\n",
        "\n",
        "synthesizer = StyleGANGenerator(\"stylegan_ffhq\").model.synthesis\n",
        "post_process = lambda image: denormalize(image).detach().cpu().numpy().astype(np.uint8)[0]\n",
        "\n",
        "pred_images = map(synthesizer, pred_dlatents)\n",
        "pred_images = map(post_process, pred_images)\n",
        "pred_images = list(map(lambda image: np.transpose(image, (1,2,0)), pred_images))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uagu6xl5e5G6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure = plt.figure(figsize=(25,10))\n",
        "columns = len(normal_images)\n",
        "rows = 2\n",
        "\n",
        "axis = []\n",
        "\n",
        "for i in range(columns):\n",
        "    axis.append(figure.add_subplot(rows, columns, i + 1))\n",
        "    axis[-1].set_title(\"Reference Image\")\n",
        "    plt.imshow(normal_images[i])\n",
        "\n",
        "for i in range(columns, columns*rows):\n",
        "    axis.append(figure.add_subplot(rows, columns, i + 1))\n",
        "    axis[-1].set_title(\"Generated With Predicted Latents\")\n",
        "    plt.imshow(pred_images[i - columns])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}